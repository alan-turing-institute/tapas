"""
In this file, we apply the Groundhog attack to synthetic networks

"""

import tapas.datasets
import tapas.generators
import tapas.threat_models
import tapas.attacks
from tapas.datasets import TUDataset

# Some fancy displays when training/testing.
import tqdm

print("Loading dataset...")

# tu_dataset = tapas.datasets.TUDataset.download_and_read("DD")
# tu_dataset = tapas.datasets.TUDataset.read("DD", "DD")
# tu_dataset = tapas.datasets.TUDataset.download_and_read("ENZYMES")
# tu_dataset2 = tapas.datasets.TUDataset.read("DD", "DD-small")
#tu_dataset = tapas.datasets.TUDataset.download_and_read("MUTAG")
tu_dataset = tapas.datasets.TUDataset.read("MUTAG", "MUTAG")

print("# of subgraph", len(tu_dataset.data))
print("# of sampled graph", len(tu_dataset.sample(frac=0.3).data))
#print("# of subgraph after drop", len(tu_dataset.drop_records(n=100, in_place=True).data))
# print("# of graph after add_records", len(tu_dataset.add_records(tu_dataset2)))

# We attack the Rng generator
generator = tapas.generators.network_generator.GNP()
print("GNP network generator......")
print("# of Rng samples:", len(generator(tu_dataset, 40)))

data_knowledge = tapas.threat_models.AuxiliaryDataKnowledge(
    # The attacker has access to 50% of the data as auxiliary information.
    # This information will be used to generate training datasets.
    tu_dataset,
    auxiliary_split=0.6,
    # The attacker knows that the real dataset contains 5000 samples. This thus
    # reflects the attacker's knowledge about the real data.
    num_training_records=76,
)
print("AuxiliaryDataKnowledge...")
print("# of aux_data", len(data_knowledge.aux_data))
print("# of test_data", len(data_knowledge.test_data))
print("# of generated datasets", len(data_knowledge.generate_datasets(50)))

sdg_knowledge = tapas.threat_models.BlackBoxKnowledge(
    generator,
    # The attacker also specifies the size of the output dataset. In practice,
    # use the size of the published synthetic dataset.
    num_synthetic_records=36,
)

# Now that we have defined the attacker's knowledge, we define their goal.
# We will here focus on a membership inference attack on a random record.
threat_model = tapas.threat_models.TargetedMIA(
    attacker_knowledge_data=data_knowledge,
    # We here select the first record, arbitrarily.
    target_record=tu_dataset.get_records([0]),
    attacker_knowledge_generator=sdg_knowledge,
    # These are mostly technical questions. They inform how the attacker will
    # be trained, but are not impactful changes of the threat model.
    #  - do we generate pairs (D, D U {target}) to train the attack?
    generate_pairs=True,
    #  - do we append the target to the dataset, or replace a record by it?
    replace_target=True,
    # (Optional) nice display for training and testing.
    iterator_tracker=tqdm.tqdm,
)

attacker = tapas.attacks.NetworkMIA()

print("Training the attack...")
# Having defined all the objects that we need, we can train the attack.
attacker.train(
    # The TargetedMIA threat model is a TrainableThreatModel: it defines a method
    #  to generate training samples (synthetic_dataset, target_in_real_dataset).
    # This is why the threat model is passed to train the attacker.
    threat_model,
    # This is the number of training pairs generated by the threat model to
    # train the attacker.
    num_samples=36,
)

print("Testing the attack...")
# The attack is trained! Evaluate it within the test model.
# [explain why we split this way.]
attack_summary = threat_model.test(attacker, num_samples=36)

# Output nice, printable metrics that evaluate the attack.
metrics = attack_summary.get_metrics()
print("Results:\n", metrics.head())
